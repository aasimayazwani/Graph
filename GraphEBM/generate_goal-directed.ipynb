{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression\n",
    "XGBClassifier\n",
    "KNeighborsClassifier\n",
    "Support Vector Classifier\n",
    "GaussianNB\n",
    "RandomForestClassifier\n",
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regular-fifth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_name {qm9,zinc250k}]\n",
      "                             [--data_dir DATA_DIR]\n",
      "                             [--property_name {qed,plogp}] [--depth DEPTH]\n",
      "                             [--add_self ADD_SELF] [--hidden HIDDEN]\n",
      "                             [--swish SWISH] [--c C] [--batch_size BATCH_SIZE]\n",
      "                             [--model_dir MODEL_DIR] [--runs RUNS]\n",
      "                             [--step_size STEP_SIZE]\n",
      "                             [--sample_step SAMPLE_STEP] [--noise NOISE]\n",
      "                             [--clamp CLAMP]\n",
      "                             [--save_result_file SAVE_RESULT_FILE]\n",
      "                             [--save_smiles SAVE_SMILES] [--save_fig SAVE_FIG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/aasimwani/Library/Jupyter/runtime/kernel-5e13db26-cd81-4526-99e1-de6f5fd4f24e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from texttable import Texttable\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from distutils.util import strtobool\n",
    "from rdkit.Chem import Draw\n",
    "import cairosvg\n",
    "from rdkit.Chem.Descriptors import qed\n",
    "\n",
    "import transform_qm9, transform_zinc250k\n",
    "from model import *\n",
    "from data_loader import NumpyTupleDataset\n",
    "from util import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import metric_random_generation, check_chemical_validity, qed, calculate_min_plogp\n",
    "\n",
    "\n",
    "### Args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_name', type=str, default='qm9', choices=['qm9', 'zinc250k'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./preprocess_data', help='Location for the dataset')\n",
    "parser.add_argument('--property_name', type=str, default='qed', choices=['qed', 'plogp'], help='Property name')\n",
    "parser.add_argument('--depth', type=int, default=2, help='Number of graph conv layers')\n",
    "parser.add_argument('--add_self', type=strtobool, default='false', help='Add shortcut in graphconv')\n",
    "parser.add_argument('--hidden', type=int, default=64, help='hidden dimension')\n",
    "parser.add_argument('--swish', type=strtobool, default='true', help='Use swish as activation function')\n",
    "parser.add_argument('--c', type=float, default=0.5, help='Dequantization using uniform distribution of [0,c)')\n",
    "parser.add_argument('--batch_size', type=int, default=10000, help='Batch size during training')\n",
    "parser.add_argument('--model_dir', type=str, default='./trained_models/qm9/epoch_1.pt', help='Location for loading checkpoints')\n",
    "parser.add_argument('--runs', type=int, default=5, help='# of runs')\n",
    "parser.add_argument('--step_size', type=int, default=10, help='Step size in Langevin dynamics')\n",
    "parser.add_argument('--sample_step', type=int, default=30, help='Number of sample step in Langevin dynamics')\n",
    "parser.add_argument('--noise', type=float, default=0.005, help='The standard variance of the added noise during Langevin Dynamics')\n",
    "parser.add_argument('--clamp', type=strtobool, default='true', help='Clamp the data/gradient during Langevin Dynamics')\n",
    "parser.add_argument('--save_result_file', type=str, default=None, help='Save evaluation result')\n",
    "parser.add_argument('--save_smiles', type=strtobool, default='true', help='Save the SMILES strings of generated melucules')\n",
    "parser.add_argument('--save_fig', type=str, default=None, help='Save the drawn figs of generated melucules? If yes, give a directory.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def tab_printer(args):\n",
    "    args = vars(args)\n",
    "    keys = sorted(args.keys())\n",
    "    t = Texttable() \n",
    "    t.set_precision(10)\n",
    "    t.add_rows([[\"Parameter\", \"Value\"]] +  [[k.replace(\"_\",\" \").capitalize(),args[k]] for k in keys])\n",
    "    print(t.draw())\n",
    "\n",
    "tab_printer(args)\n",
    "\n",
    "\n",
    "### Code adapted from https://github.com/rosinality/igebm-pytorch\n",
    "\n",
    "def requires_grad(parameters, flag=True):\n",
    "    for p in parameters:\n",
    "        p.requires_grad = flag\n",
    "\n",
    "        \n",
    "def clip_grad(parameters, optimizer):\n",
    "    with torch.no_grad():\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = optimizer.state[p]\n",
    "\n",
    "                if 'step' not in state or state['step'] < 1:\n",
    "                    continue\n",
    "\n",
    "                step = state['step']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                _, beta2 = group['betas']\n",
    "\n",
    "                bound = 3 * torch.sqrt(exp_avg_sq / (1 - beta2 ** step)) + 0.1\n",
    "                p.grad.data.copy_(torch.max(torch.min(p.grad.data, bound), -bound))\n",
    "\n",
    "                \n",
    "                \n",
    "def generate(model, n_atom, n_atom_type, n_edge_type, device):\n",
    "    parameters = model.parameters()\n",
    "    ### Langevin dynamics\n",
    "    gen_x = torch.rand(args.batch_size, n_atom, n_atom_type, device=device) * (1 + args.c) # (10000, 9, 5)\n",
    "    gen_adj = torch.rand(args.batch_size, n_edge_type, n_atom, n_atom, device=device) #(10000, 4, 9, 9)\n",
    "    \n",
    "        \n",
    "    gen_x.requires_grad = True\n",
    "    gen_adj.requires_grad = True\n",
    "    \n",
    "\n",
    "\n",
    "    requires_grad(parameters, False)\n",
    "    model.eval()\n",
    "    \n",
    "    noise_x = torch.randn(gen_x.shape[0], n_atom, n_atom_type, device=device)  # (10000, 9, 5)\n",
    "    noise_adj = torch.randn(gen_adj.shape[0], n_edge_type, n_atom, n_atom, device=device)  #(10000, 4, 9, 9) \n",
    "\n",
    "    for k in range(args.sample_step):\n",
    "\n",
    "        noise_x.normal_(0, args.noise)\n",
    "        noise_adj.normal_(0, args.noise)\n",
    "        gen_x.data.add_(noise_x.data)\n",
    "        gen_adj.data.add_(noise_adj.data)\n",
    "        \n",
    "\n",
    "        gen_out = model(gen_adj, gen_x)\n",
    "        gen_out.sum().backward()\n",
    "        if args.clamp:\n",
    "            gen_x.grad.data.clamp_(-0.01, 0.01)\n",
    "            gen_adj.grad.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "\n",
    "        gen_x.data.add_(gen_x.grad.data, alpha=-args.step_size)\n",
    "        gen_adj.data.add_(gen_adj.grad.data, alpha=-args.step_size)\n",
    "\n",
    "        gen_x.grad.detach_()\n",
    "        gen_x.grad.zero_()\n",
    "        gen_adj.grad.detach_()\n",
    "        gen_adj.grad.zero_()\n",
    "        \n",
    "        gen_x.data.clamp_(0, 1 + args.c)\n",
    "        gen_adj.data.clamp_(0, 1)\n",
    "\n",
    "    gen_x = gen_x.detach()\n",
    "    gen_adj = gen_adj.detach()\n",
    "    \n",
    "    gen_adj = gen_adj + gen_adj.permute(0, 1, 3, 2)    # A+A^T is a symmetric matrix\n",
    "    gen_adj = gen_adj / 2\n",
    "    \n",
    "   \n",
    "    return gen_x, gen_adj   # (10000, 9, 5), (10000, 4, 9, 9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    ### Load dataset\n",
    "    if args.data_name==\"qm9\":\n",
    "        atomic_num_list = [6, 7, 8, 9, 0]\n",
    "        n_atom_type = 5\n",
    "        n_atom = 9\n",
    "        n_edge_type = 4\n",
    "    elif args.data_name==\"zinc250k\":\n",
    "        atomic_num_list = transform_zinc250k.zinc250_atomic_num_list\n",
    "        n_atom_type = len(atomic_num_list) #10\n",
    "        n_atom = 38\n",
    "        n_edge_type = 4\n",
    "    else:\n",
    "        print(\"This dataset name is not supported!\")\n",
    "\n",
    "\n",
    "    ### Load trained model\n",
    "    model = GraphEBM(n_atom_type, args.hidden, n_edge_type, args.swish, args.depth, add_self = args.add_self)\n",
    "    print(\"Loading paramaters from {}\".format(args.model_dir))\n",
    "    model.load_state_dict(torch.load(args.model_dir))\n",
    "    model = model.to(device)\n",
    "    \n",
    "\n",
    "    t_end = time.time()\n",
    "\n",
    "    print('Load trained model and data done! Time {:.2f} seconds'.format(t_end - t_start))\n",
    "    print('==========================================')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ### Random generation\n",
    "    gen_time = []\n",
    "    valid_ratio = []\n",
    "    \n",
    "    t_start = time.time()\n",
    "    gen_x, gen_adj = generate(model, n_atom, n_atom_type, n_edge_type, device)\n",
    "    \n",
    "    gen_mols = gen_mol(gen_adj, gen_x, atomic_num_list, correct_validity=True)\n",
    "    gen_results = metric_random_generation(gen_mols)\n",
    "\n",
    "    t_end = time.time()\n",
    "\n",
    "    gen_time.append(t_end - t_start)\n",
    "\n",
    "    valid_ratio.append(gen_results['valid_ratio'])\n",
    "    \n",
    "    valid_mols = [mol for mol in gen_mols if check_chemical_validity(mol)]\n",
    "\n",
    "     \n",
    "    if args.property_name=='qed':\n",
    "        prop_scores = [qed(mol) for mol in valid_mols]\n",
    "    elif args.property_name=='plogp':\n",
    "        prop_scores = [calculate_min_plogp(mol) for mol in valid_mols]\n",
    "\n",
    "\n",
    "    inds = sorted(range(len(prop_scores)), key=lambda k: prop_scores[k], reverse=True)\n",
    "    gen_smiles = [Chem.MolToSmiles(mol, isomericSmiles=False) for mol in gen_mols]\n",
    "    sorted_smiles = [gen_smiles[i] for i in inds]\n",
    "    sorted_scores = [prop_scores[i] for i in inds]\n",
    "        \n",
    "    if args.save_result_file is not None:\n",
    "        file = args.save_result_file\n",
    "        with open(file, 'a+') as f:\n",
    "            f.write('Model: '+args.model_dir+'\\n')\n",
    "            f.write('Property:'+args.property_name+'\\n')\n",
    "            f.write('Validity: '+str(np.mean(valid_ratio))+'±'+str(np.std(valid_ratio))+'\\n')\n",
    "            f.write('Average score: '+ str(np.mean(sorted_scores)) + '±' + str(np.std(sorted_scores)) + '\\n')\n",
    "            if args.save_smiles:\n",
    "                f.write(str(sorted_smiles)+'\\n')\n",
    "                f.write(str(sorted_scores)+'\\n')\n",
    "            f.write('\\n')\n",
    "        \n",
    "    if args.save_fig is not None:\n",
    "        gen_dir = args.save_fig\n",
    "        os.makedirs(gen_dir, exist_ok=True)\n",
    "        for i in range(len(gen_mols)):\n",
    "            filepath = os.path.join(gen_dir, 'generated_mols_{}.png'.format(i+1))\n",
    "            img = Draw.MolToImage(gen_mols[i])\n",
    "            img.save(filepath)\n",
    "\n",
    "        \n",
    "    print('10 highest score:', sorted_scores[0:10])\n",
    "    print('==========================================')\n",
    "    print('Average score:', str(np.mean(sorted_scores)), '±', str(np.std(sorted_scores)))\n",
    "    print('==========================================')\n",
    "    print(\"Validity: {:.3f}% ± {:.3f}%, vals={}\".format(np.mean(valid_ratio), np.std(valid_ratio), valid_ratio))\n",
    "    print('------------------------------------------')\n",
    "    print(\"Generation Time: {:.3f} ± {:.3f} seconds, vals={}\".format(np.mean(gen_time), np.std(gen_time), gen_time))\n",
    "    print('==========================================')\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-discretion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
